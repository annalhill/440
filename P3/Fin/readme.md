# Anna Louise Hill
## Fin (Final Draft of Project 3)

## Title: Talking without Words, Cybernetic Communication, Vibiz
## Date: April 25, 2018
### University of Montana Interactive Performance Spring 2018

1. Related Works:
I really wanted to find as many works out there as I could to inspire me not only in visual aesthetics but also what I am capable of creating in P5.js, a relatively new program for me. Luckily, there are a lot of open source resources out there for me to reference. Below are a few of the sources I've explored and what intrigued me about them as well as what I would take away from them to incorporate in my own visualizer.

- B Lack, Open Processing P5.js Sketch, Circular Processing Network, September 25, 2013, Visited April 14, 2018
https://www.openprocessing.org/sketch/111888
- Tina Anastopoulos, P5.js sketch, Fan Loader, Visited April 10, 2018
https://www.kode.to/TWAIN/fan-loader-mWjPKm
- Nairako, Visualization art using p5.js based on the sound of waves at Yuigahama 1 of 2, November 7, 2015, Visited April 16, 2018
https://www.youtube.com/watch?v=mBPAtibRb1E

2. Initial Goals:
Brainstorming going into the project:
- My first idea is to continue with Project two and expand upon the word bank that I was creating for the project, Sound Poetry. This would mean expanding the number of words that I can input into the system as well as creating a more visually inviting display.
- The second idea I am adapting from Week 8 in the Creative Coding class. In the lesson we worked with loops and visual effects based on if, else statements. I would like to continue working with sound input and the ways it can alter and affect visuals.

Ideas as I start project three:
- Visualizer
- Use of colors and movement to create an interesting and dynamic Project
- Working in P5.js
- Use of shapes to guide the visuals
  - Art inside of various shapes?
  - Use of shapes to create an entire composition?
  - "Drawing"?

3. Final Goals and Concept:
The idea behind this project was simply to try and find a visual aid for audio input via beat detection, and amplitude scale. The project initially was going to react to the performer and their choice of performance but the idea has developed into how communication between technology and human interaction can influence each other. One way to do this is to take the performer and have them play whatever they want with the visual components playing behind them, this would involve very little interaction. The better idea is to have the performer and the screen looking at each other. The program will continuously be listening to the audio input and reacting to the sounds it's intaking. On the other side, the performer will be watching the visuals and reacting to them. If there are lighter colors, maybe play a softer sound, darker colors, a heavier sound, etc.
